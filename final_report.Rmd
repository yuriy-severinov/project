---
title: "Who will buy our new product"
author: "Severinov Yuriy, Chernysheva Mariya, Makhmutshina Kamila"
date: "6/20/2021"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, warning=FALSE, error=FALSE, message=FALSE}
library(ggplot2) # Basic plot making library
library(GGally) # To use ggpairs
library(ggfortify) # To use autoplot
library(gridExtra) # To make numbers of graphics in one space
library(RColorBrewer) # To color graphics easier
library(reshape2) # To make heatmap
library(modelr) # To make logistic regression model
library(dplyr) # To use data manipulation tools
library(scales) # Makes maling graphics easier. Процентный формат для осей ggplot
library(memisc) # Stack of tools. To work with tables for us??
library(tidyverse) # Трансформация и визуализация данных
library(rpart) # Деревья в R: recursive partitioning
library(C50) # Деревья в R: C5.0
library(party) # Деревья в R: conditional inference
library(rpart.plot) # Визуализация деревьев
library(partykit) # Визуализация деревьев
library(rattle) # Provides a graphical user interface to very many other R packages
library(mlr) # Фреймворк для машинного обучения, здесь используется для подсчета показателей ошибки
library(ROCR) # Построение ROC-кривых
library(forcats) # Работа с факторными переменными
library(parallelMap) # распараллеливание задач
library(nnet) # Fitting neural networks
library(randomForest) # Modeling Random Forests
library(h2o) # Modeling h2o
library(xgboost) # Modeling XGBoost
library(mltools) # To convert categorical variables into numerical
library(data.table) # To convert categorical variables into numerical
library(GenSA) # To tune Random Forest model 
```


# Task


The April 2015 Nepal earthquake (also known as the Gorkha earthquake) killed nearly 9,000 people and injured nearly 22,000. It occurred at 11:56 Nepal Standard Time on 25 April 2015, with a magnitude of 7.8 Mw or 8.1Ms and a maximum Mercalli Intensity of VIII (Severe).


Almost 9,000 lives were lost, millions of people were instantly made homeless, and $10 billion in damages––about half of Nepal's nominal GDP––were incurred. In the years since, the Nepalese government has worked intensely to help rebuild the affected districts' infrastructures. Throughout this process, the National Planning Commission, along with Kathmandu Living Labs and the Central Bureau of Statistics, has generated one of the largest post-disaster datasets ever collected, containing valuable information on earthquake impacts, household conditions, and socio-economic-demographic statistics.


From the portal about earthquake:

> Following the 7.8 Mw Gorkha Earthquake on April 25, 2015, Nepal carried out a massive household survey using mobile technology to assess building damage in the earthquake-affected districts. Although the primary goal of this survey was to identify beneficiaries eligible for government assistance for housing reconstruction, it also collected other useful socio-economic information. In addition to housing reconstruction, this data serves a wide range of uses and users e.g. researchers, newly formed local governments, and citizens at large. The purpose of this portal is to open this data to the public.


**We're need to predict the ordinal variable `damage_grade`, which represents a level of damage to the building that was hit by the earthquake**. There are 3 grades of the damage:

  - **1** - represents low damage
  - **2** - represents a medium amount of damage
  - **3** - represents almost complete destruction


The dataset mainly consists of information on the buildings' structure and their legal ownership. Each row in the dataset represents a specific building in the region that was hit by Gorkha earthquake.

There are 39 columns in this dataset, where the `building_id` column is a unique and random identifier. The remaining 38 features are described in the section below. Categorical variables have been obfuscated random lowercase ascii characters. The appearance of the same character in distinct columns does not imply the same original value.

Description of variables:

 - `geo_level_1_id, geo_level_2_id, geo_level_3_id` (type: int): geographic region in which building exists, from largest (level 1) to most specific sub-region (level 3). Possible values: level 1: 0-30, level 2: 0-1427, level 3: 0-12567.
 
 - `count_floors_pre_eq` (type: int): number of floors in the building before the earthquake.
 
 - `age` (type: int): age of the building in years.

 - `area_percentage` (type: int): normalized area of the building footprint.

 - `height_percentage` (type: int): normalized height of the building footprint.

 - `land_surface_condition` (type: categorical): surface condition of the land where the building was built. Possible values: n, o, t.

 - `foundation_type` (type: categorical): type of foundation used while building. Possible values: h, i, r, u, w.

 - `roof_type` (type: categorical): type of roof used while building. Possible values: n, q, x.

 - `ground_floor_type` (type: categorical): type of the ground floor. Possible values: f, m, v, x, z.

 - `other_floor_type` (type: categorical): type of constructions used in higher than the ground floors (except of roof). Possible values: j, q, s, x.

 - `position` (type: categorical): position of the building. Possible values: j, o, s, t.

 - `plan_configuration` (type: categorical): building plan configuration. Possible values: a, c, d, f, m, n, o, q, s, u.

 - `has_superstructure_adobe_mud` (type: binary): flag variable that indicates if the superstructure was made of Adobe/Mud.

 - `has_superstructure_mud_mortar_stone` (type: binary): flag variable that indicates if the superstructure was made of Mud Mortar - Stone.

 - `has_superstructure_stone_flag` (type: binary): flag variable that indicates if the superstructure was made of Stone.

 - `has_superstructure_cement_mortar_stone` (type: binary): flag variable that indicates if the superstructure was made of Cement Mortar - Stone.

 - `has_superstructure_mud_mortar_brick` (type: binary): flag variable that indicates if the superstructure was made of Mud Mortar - Brick.

 - `has_superstructure_cement_mortar_brick` (type: binary): flag variable that indicates if the superstructure was made of Cement Mortar - Brick.

 - `has_superstructure_timber` (type: binary): flag variable that indicates if the superstructure was made of Timber.

 - `has_superstructure_bamboo` (type: binary): flag variable that indicates if the superstructure was made of Bamboo.

 - `has_superstructure_rc_non_engineered` (type: binary): flag variable that indicates if the superstructure was made of non-engineered reinforced concrete.

 - `has_superstructure_rc_engineered` (type: binary): flag variable that indicates if the superstructure was made of engineered reinforced concrete.

 - `has_superstructure_other` (type: binary): flag variable that indicates if the superstructure was made of any other material.

 - `legal_ownership_status` (type: categorical): legal ownership status of the land where building was built. Possible values: a, r, v, w.

 - `count_families` (type: int): number of families that live in the building.

 - `has_secondary_use` (type: binary): flag variable that indicates if the building was used for any secondary purpose.

 - `has_secondary_use_agriculture` (type: binary): flag variable that indicates if the building was used for agricultural purposes.

 - `has_secondary_use_hotel` (type: binary): flag variable that indicates if the building was used as a hotel.

 - `has_secondary_use_rental` (type: binary): flag variable that indicates if the building was used for rental purposes.

 - `has_secondary_use_institution` (type: binary): flag variable that indicates if the building was used as a location of any institution.

 - `has_secondary_use_school` (type: binary): flag variable that indicates if the building was used as a school.

 - `has_secondary_use_industry` (type: binary): flag variable that indicates if the building was used for industrial purposes.

 - `has_secondary_use_health_post` (type: binary): flag variable that indicates if the building was used as a health post.

 - `has_secondary_use_gov_office` (type: binary): flag variable that indicates if the building was used fas a government office.

 - `has_secondary_use_use_police` (type: binary): flag variable that indicates if the building was used as a police station.

 - `has_secondary_use_other` (type: binary): flag variable that indicates if the building was secondarily used for other purposes.


# Solution


## Loading the data


First of all, let's import dataset and merge it with it's values of damage.


```{r, warning=FALSE, error=FALSE, message=FALSE}
variables <- read_csv('Train_Values.csv')
values <- read_csv('Train_Labels.csv')
earthquake <- merge(variables, values, by="building_id")
```


Then we need to explore our data.


## Exploratory data analysis


First of all we want to see correlations between variables. For that we make ggpairs plots, using only informative variables, colored by the level of damage. We use test subset to make plots easier to draw.


```{r, warning=FALSE, error=FALSE, message=FALSE}
ggpairs(data = earthquake[c("damage_grade", "count_floors_pre_eq", "age", "area_percentage" , "height_percentage", "count_families")],
        title='relations between varibles',
        mapping=ggplot2::aes(colour = as.factor(damage_grade)))
```


To see correlations between variables, we dropped category variables and visualize them on heatmap.


```{r, warning=FALSE, error=FALSE, message=FALSE}
cormat <- melt(round(cor(earthquake %>% dplyr::select(-c('land_surface_condition', 'foundation_type', 'roof_type', 'ground_floor_type', 'other_floor_type', 'position', 'plan_configuration', 'land_surface_condition', 'legal_ownership_status', 'damage_grade'))), 2))
ggplot(data = cormat, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile() +
  theme(axis.text.x = element_text(angle = 90), aspect.ratio=9/10)
```


What we can say, looking on heatmap?
   - correlation between variables is very weak;
   - has_secondary_use correlating with it's parts;
   - height_percentage is highly correlating with count_floors_pre_eq;
   - area_percentage and height_percentage correlating with has_super_structure features and secondary use of buildings.


Let's look closer to the damage distribution.


```{r, warning=FALSE, error=FALSE, message=FALSE}
ggplot(data=earthquake,
       aes(x=as.factor(damage_grade))) +
       geom_bar()
```


We see, that the earthquake bring a lot of medium damage and near the third part of buildings was totally destroyed. 


We predict that pretty old buildings can be much more sensitive to earthquakes, but the ancients of them could see more dangerous disasters and survived through them, so let's see the distribution of age of buildings across the level of damage.


```{r, warning=FALSE, error=FALSE, message=FALSE}
p1 <- ggplot(earthquake) +
  geom_histogram(aes(x = age, fill = as.factor(damage_grade)), bins = 200) +
  scale_x_continuous()
p2 <- ggplot(earthquake) +
  geom_histogram(aes(x = age, fill = as.factor(damage_grade)), bins = 24) +
  scale_x_continuous(limits = c(-12, 110), breaks = seq(0, 110, 5))
p3 <- ggplot(earthquake) +
  geom_histogram(aes(x = age, fill = as.factor(damage_grade)), bins = 22) +
  scale_x_continuous(limits = c(950, 1050)) +
  theme(legend.position='none')
p4 <- ggplot(earthquake) +
  geom_histogram(aes(x = age, fill = as.factor(damage_grade)), bins = 22) +
  scale_x_continuous(limits = c(-1, 2)) +
  theme(legend.position='none')
grid.arrange(p1, p2, grid.arrange(p4, p3, ncol = 2), ncol = 1)
```


We decided to look closer to the proportions on damage across the age of building.


```{r, warning=FALSE, error=FALSE, message=FALSE}
ggplot(earthquake) +
  geom_bar(aes(x = age, fill = as.factor(damage_grade)), position = "fill") +
  scale_x_continuous(limits = c(-12, 110))
```


On that plots we can see that our prediction was wrong and less older buildings got injured. But the older buildings have more damage, than the newer ones. The distribution is skewed to the right.

We can see strange appendix in the right part of plot. It seems that lots of ancient buildings have age of 1000 years in documents, despite of their real age.

Other strange observation is that lots of buildings have age of 0. The next step after 0 is 5. It seems that buildings younger that 4 years old have been labeled as new in documents. And only that buildings have lesser hard damage grade than low one.


Then let's see on the `area_percentage`, `height_percentage` and `count_families` variables. It seems that dustribution of them close to be normal.


```{r, warning=FALSE, error=FALSE, message=FALSE}
p1 <- ggplot(earthquake) +
  geom_histogram(aes(x = area_percentage,
                     fill = as.factor(damage_grade)), bins = 26) +
  scale_x_continuous(limits = c(0, 25), breaks = seq(0, 25, 1))
p2 <- ggplot(earthquake) +
  geom_histogram(aes(x = height_percentage,
                     fill = as.factor(damage_grade)), bins = 15) +
  scale_x_continuous(limits = c(0, 14), breaks = seq(0, 14, 1))
p3 <- ggplot(earthquake) +
  geom_histogram(aes(x = count_families,
                     fill = as.factor(damage_grade)), bins = 8) +
  scale_x_continuous(limits = c(-1, 5), breaks = seq(-1, 5, 1))
grid.arrange(p1, p2, p3, ncol = 1)
```


Then we want to see, how numbers of floors impacts on the destruction probability. We predict that higher building can be less sustainable to the earthquake, but in the other hand, they can have more progressive and effective solution in construction to resist the disasters. So let's see on the plot.


```{r, warning=FALSE, error=FALSE, message=FALSE}
ggplot(earthquake) +
  geom_bar(aes(x = count_floors_pre_eq,
                     fill = as.factor(damage_grade)))
```


We see that percent of seriously damaged higher buildings is much more than of the lower ones.


How we can obscure the categorical variables.


```{r, warning=FALSE, error=FALSE, message=FALSE}
p1 <- ggplot(data = earthquake,
             aes(x = land_surface_condition,
                 fill = as.factor(damage_grade))) +
  geom_bar() + theme(legend.position="bottom")
p2 <- ggplot(data = earthquake,
             aes(x = foundation_type,
                 fill = as.factor(damage_grade))) +
  geom_bar() + theme(legend.position="bottom")
p3 <- ggplot(data = earthquake,
             aes(x = roof_type,
                 fill = as.factor(damage_grade))) +
  geom_bar() + theme(legend.position='none')
p4 <- ggplot(data = earthquake,
             aes(x = ground_floor_type,
                 fill = as.factor(damage_grade))) +
  geom_bar() + theme(legend.position='none')
p5 <- ggplot(data = earthquake,
             aes(x = other_floor_type,
                 fill = as.factor(damage_grade))) +
  geom_bar() + theme(legend.position="bottom")
p6 <- ggplot(data = earthquake,
             aes(x = position,
                 fill = as.factor(damage_grade))) +
  geom_bar() + theme(legend.position="bottom")
p7 <- ggplot(data = earthquake,
             aes(x = plan_configuration,
                 fill = as.factor(damage_grade))) +
  geom_bar() + theme(legend.position='none')
p8 <- ggplot(data = earthquake,
             aes(x = legal_ownership_status,
                 fill = as.factor(damage_grade))) +
  geom_bar() + theme(legend.position='none')

grid.arrange(p1, p2, p3, p4, ncol = 2)
grid.arrange(p5, p6, p7, p8, ncol = 2)
```


We expect that depending of age, some of variables can behave different, because of different constructions, reconstructions and overhauls.


```{r, warning=FALSE, error=FALSE, message=FALSE}
ggplot(earthquake) +
  geom_bar(aes(x = age, fill = as.factor(damage_grade)), position = "fill") +
  scale_x_continuous(limits = c(-12, 110)) +
  facet_wrap(~ count_floors_pre_eq)
```


```{r, warning=FALSE, error=FALSE, message=FALSE}
ggplot(earthquake) +
  geom_point(aes(x = age, y = area_percentage, color = as.factor(damage_grade))) +
  scale_x_continuous(limits = c(-12, 110))
```


```{r, warning=FALSE, error=FALSE, message=FALSE}
ggplot(earthquake) +
  geom_point(aes(x = age, y = height_percentage, color = as.factor(damage_grade))) +
  scale_x_continuous(limits = c(-12, 110))
```


Then we decided that depending of numbers of floors area and height can behave different, so we decided to explore them too.


```{r, warning=FALSE, error=FALSE, message=FALSE}
ggplot(earthquake) +
  geom_histogram(aes(x = area_percentage, fill = as.factor(damage_grade))) +
  scale_x_continuous(limits = c(0, 50)) +
  facet_wrap(~ count_floors_pre_eq, scales = "free_y")
```


```{r, warning=FALSE, error=FALSE, message=FALSE}
ggplot(earthquake) +
  geom_histogram(aes(x = height_percentage, fill = as.factor(damage_grade))) +
  scale_x_continuous(limits = c(0, 30)) +
  facet_wrap(~ count_floors_pre_eq, scales = "free_y")
```




Then we can expect that more populus buildings will be less qualitative, more cheap, so less sustainable.


```{r, warning=FALSE, error=FALSE, message=FALSE}
ggplot(earthquake) +
  geom_histogram(aes(x = area_percentage, fill = as.factor(damage_grade))) +
  scale_x_continuous(limits = c(0, 75)) +
  facet_wrap(~ count_families, scales = "free_y")
```


```{r, warning=FALSE, error=FALSE, message=FALSE}
ggplot(earthquake) +
  geom_histogram(aes(x = height_percentage, fill = as.factor(damage_grade))) +
  scale_x_continuous(limits = c(0, 30)) +
  facet_wrap(~ count_families, scales = "free_y")
```


As we see, depending on age, count of floors, families, variables really can behave in different way. We'll implement their interactions in dataset and it could help us to get the better result.


What we conclude after exploring our data:

 - `area_percentage` and `height_percentage` are correlating;
 - has_secondary_use correlating with it's parts;
 - height_percentage is highly correlating with count_floors_pre_eq;
 - area_percentage and height_percentage correlating with has_super_structure features and secondary use of buildings;
 - the older buildings had more damage, than the newer ones, but more newer buildings was damaged;
 - percent of seriously damaged higher buildings is much more than of the lower ones;
 - depending on age, count of floors, families, variables really can behave in different way.
 


## Preparing the data


Let's see, what data we have in our dataset.


```{r, warning=FALSE, error=FALSE, message=FALSE}
str(earthquake)
```


Models can work worse if the dataset have missing values. So then we checked data for them.


```{r, warning=FALSE, error=FALSE, message=FALSE}
dim(earthquake %>%
      filter(is.na(earthquake)))
```


So we see that dataset is clear from missing values, we can move on.


Some of the models are not able to work with categorical variables. To solve that, we decided to convert them into factors ones.


```{r, warning=FALSE, error=FALSE, message=FALSE}
variables$land_surface_condition <- factor(variables$land_surface_condition)
variables$foundation_type <- factor(variables$foundation_type)
variables$roof_type <- factor(variables$roof_type)
variables$ground_floor_type <- factor(variables$ground_floor_type)
variables$other_floor_type <- factor(variables$other_floor_type)
variables$position <- factor(variables$position)
variables$plan_configuration <- factor(variables$plan_configuration)
variables$legal_ownership_status <- factor(variables$legal_ownership_status)
```


We found that `area_percentage` and `height_percentage` are correlating, so we can implement interaction between them to make the prediction more accuracy


```{r, warning=FALSE, error=FALSE, message=FALSE}
variables$height_area_interaction <- variables$height_percentage*variables$area_percentage
```


Then we are going to add interactions between age, count of floors, families and other variables by adding new variables in the following way:
 

 - count_of_floors_per_age <- count_floors_pre_eq/(age+0.001) 
 - count_of_floors_per_area <- count_floors_pre_eq/area_percentage 
 - count_of_floors_per_height <- count_floors_pre_eq/height_percentage 
 - area_per_age <- area_percentage/(age+0.001) 
 - height_per_age <- height_percentage/(age+0.001) 
 - families_on_floor <- count_families/count_floors_pre_eq 
 - families_on_area <- count_families/area_percentage 
 - families_on_height <- count_families/height_percentage


As you see, we added an miserable constant for age variable. It is to prevent errors while trying to divide by zero. As we remember, some of the newest buildings nave age of 0.


```{r, warning=FALSE, error=FALSE, message=FALSE}
variables$count_of_floors_per_age <- variables$count_floors_pre_eq/(variables$age+0.001) 
variables$count_of_floors_per_area <- variables$count_floors_pre_eq/variables$area_percentage 
variables$count_of_floors_per_height <- variables$count_floors_pre_eq/variables$height_percentage 
variables$area_per_age <- variables$area_percentage/(variables$age+0.001) 
variables$height_per_age <- variables$height_percentage/(variables$age+0.001) 
variables$families_on_floor <- variables$count_families/variables$count_floors_pre_eq 
variables$families_on_area <- variables$count_families/variables$area_percentage 
variables$families_on_height <- variables$count_families/variables$height_percentage
```


To optimize working of models, let's convert them into dummies.


```{r, warning=FALSE, error=FALSE, message=FALSE}
variables_dummy <- as.data.frame(one_hot(as.data.table(variables)))
```


```{r, warning=FALSE, error=FALSE, message=FALSE}
earthquake_prepared <- merge(variables_dummy, values, by="building_id")
earthquake_prepared$damage_grade <- factor(earthquake_prepared$damage_grade)
```


Beyond that we found that volume of data is extremely large and modeling takes lots of time. To reduce risks, we decided to take 10% part of our dataset and benchmark models on it.


```{r, warning=FALSE, error=FALSE, message=FALSE}
set.seed(375122)
test_ids <- sample.int(nrow(earthquake_prepared), size = nrow(earthquake_prepared) * 0.1)
earthquake_benchmark <- earthquake_prepared[test_ids, ]
```


As the most common value of target variable `damage_grade`, medium amount of damage will be used by us as a basic value. So we Re-level dataset by that value.


```{r, warning=FALSE, error=FALSE, message=FALSE}
earthquake_benchmark$damage_grade <- relevel(earthquake_benchmark$damage_grade, ref = '2')
```


Now we are ready to make our model.


## Making models


While choosing the model, we faced that logistic regression is not able to predict ordial variable. Searching the solution of that problem, we found multinomial and ordinal regression. Ordinal regression works well while distance between points of predictions are not equal. In our situation, when we don't know the economic difference between three grades of damage, we decided to concentrate on multinomial regression.


Except them we will use familiar to us decision trees and xgboost package, which is boosting decision trees.


To automatize the modeling process, we decided to use `mlr` package with next set of models:

  - classif.randomForest
  - classif.xgboost
  - classif.multinom
  - classif.rpart
  - classif.ctree
  - classif.C50
  

```{r, warning=FALSE, error=FALSE, message=FALSE}
earthquake_benchmark_task <- makeClassifTask(id = "Grade of Damage", 
                         data = earthquake_benchmark, target = "damage_grade",
                         fixup.data = "warn")
```


Then we make learner.


```{r, warning=FALSE, error=FALSE, message=FALSE}
lrn_benchmark_mlr <- makeLearners(cls = c("randomForest", "xgboost", "multinom", "rpart", "ctree", "C50"),
               ids = c("RandomForest", "XGBoost", "Multinom", "rpart", "ctree", "C5.0"),
               type = "classif", predict.type = "response")
```


We want to use cross-validation to get the best model beyond them.


```{r, warning=FALSE, error=FALSE, message=FALSE}
earthquake_rdesc <- makeResampleDesc(method = "CV", 
                                   stratify = TRUE,
                                   iter = 10)
```


Then we're trying to train our benchmark model. We can't use F-1 score in `mlr` package to measure the quality of multinomial model, so we will rely on kappa and accuracy to choose the best model. It seems that there is no easy way to measure macro F-1 score in R.


```{r, warning=FALSE, error=FALSE, message=FALSE}
clas_ms = list(acc, kappa, mmce)

set.seed(16117, "L'Ecuyer")

num_cores <- parallel::detectCores()
parallelStartSocket(num_cores)

earthquake_bench <- lrn_benchmark_mlr %>%
  benchmark(tasks = earthquake_benchmark_task,
         resampling = earthquake_rdesc,
         measures = append(list(timetrain), clas_ms),
         show.info = FALSE)

parallelStop()

```


```{r, warning=FALSE, error=FALSE, message=FALSE}
earthquake_bench %>%
  getBMRAggrPerformances(as.df = TRUE) %>%
  arrange(desc(acc.test.mean)) %>%
  dplyr::select(-c(task.id)) %>%
  mutate_if(is.numeric, round, digits = 3)
```


As we see, the Random Forest, rpart, C5.0 and XGBoost have nearly same not really high result. Let's try them on the larger amount of data.


```{r, warning=FALSE, error=FALSE, message=FALSE}
set.seed(375122)
test_ids <- sample.int(nrow(earthquake_benchmark), size = nrow(earthquake_benchmark) * 0.5)
earthquake_benchmark_2 <- earthquake_benchmark[test_ids, ]
earthquake_benchmark_2$damage_grade <- relevel(earthquake_benchmark_2$damage_grade, ref = '2')

earthquake_benchmark_task_2 <- makeClassifTask(id = "Grade of Damage", 
                         data = earthquake_benchmark_2, target = "damage_grade",
                         fixup.data = "warn")

lrn_benchmark_mlr_2 <- makeLearners(cls = c("randomForest", "xgboost", "rpart", "C50"),
               ids = c("RandomForest", "XGBoost", "rpart", "C5.0"),
               type = "classif", predict.type = "response")

set.seed(16117, "L'Ecuyer")

num_cores <- parallel::detectCores()
parallelStartSocket(num_cores)

earthquake_bench_2 <- lrn_benchmark_mlr_2 %>%
  benchmark(tasks = earthquake_benchmark_task_2,
         resampling = earthquake_rdesc,
         measures = append(list(timetrain), clas_ms),
         show.info = FALSE)

parallelStop()

earthquake_bench_2 %>%
  getBMRAggrPerformances(as.df = TRUE) %>%
  arrange(desc(acc.test.mean)) %>%
  dplyr::select(-c(task.id)) %>%
  mutate_if(is.numeric, round, digits = 3)
```


As we see, Random Forest shows the best quality indicators. So, we train Random Forest model on all amount of data.


```{r, warning=FALSE, error=FALSE, message=FALSE}
earthquake_task <- makeClassifTask(id = "Grade of Damage",
                                   data = earthquake_prepared, target = "damage_grade",
                                   fixup.data = "warn")

lrn_rf <- makeLearner(cl = "classif.randomForest",
                      id = "RandomForest",
                      predict.type = "response")

earthquake_rf <- train(learner = lrn_rf,
                       task = earthquake_task)
```


We saved that model into external file, so we won't have to make it twice.


```{r, warning=FALSE, error=FALSE, message=FALSE}
saveRDS(earthquake_rf, "Random Forest on all data")
```


We tried to tune our model, but we have no such powerful machine to do that. Even after night of working it didn't show resuult.


Now we are ready to predict competition test data. To predict correctly, we need to make the same transformations with data, as with training one.


```{r, warning=FALSE, error=FALSE, message=FALSE}
earthquake_rf <- readRDS("Random Forest on all data")
data_to_predict <- read.csv('Test_Values.csv')

data_to_predict$land_surface_condition <- factor(data_to_predict$land_surface_condition)
data_to_predict$foundation_type <- factor(data_to_predict$foundation_type)
data_to_predict$roof_type <- factor(data_to_predict$roof_type)
data_to_predict$ground_floor_type <- factor(data_to_predict$ground_floor_type)
data_to_predict$other_floor_type <- factor(data_to_predict$other_floor_type)
data_to_predict$position <- factor(data_to_predict$position)
data_to_predict$plan_configuration <- factor(data_to_predict$plan_configuration)
data_to_predict$legal_ownership_status <- factor(data_to_predict$legal_ownership_status)

data_to_predict$height_area_interaction <- data_to_predict$height_percentage*data_to_predict$area_percentage

data_to_predict$count_of_floors_per_age <- data_to_predict$count_floors_pre_eq/(data_to_predict$age+0.001) 
data_to_predict$count_of_floors_per_area <- data_to_predict$count_floors_pre_eq/data_to_predict$area_percentage 
data_to_predict$count_of_floors_per_height <- data_to_predict$count_floors_pre_eq/data_to_predict$height_percentage 
data_to_predict$area_per_age <- data_to_predict$area_percentage/(data_to_predict$age+0.001) 
data_to_predict$height_per_age <- data_to_predict$height_percentage/(data_to_predict$age+0.001) 
data_to_predict$families_on_floor <- data_to_predict$count_families/data_to_predict$count_floors_pre_eq 
data_to_predict$families_on_area <- data_to_predict$count_families/data_to_predict$area_percentage 
data_to_predict$families_on_height <- data_to_predict$count_families/data_to_predict$height_percentage

data_to_predict <- as.data.frame(one_hot(as.data.table(data_to_predict)))
```


```{r, warning=FALSE, error=FALSE, message=FALSE}
data_to_predict$damage_grade <- getPredictionResponse(predict(earthquake_rf, newdata = data_to_predict, type = "response"))
to_predict <- to_predict %>% dplyr::select(building_id, damage_grade)
write.csv(to_predict,'submission.csv', row.names = FALSE)
```


After all of our transformations and using Random Forest, we perform result of the 71% of F-1 score. We think that result is pretty good for our model, trained on low-perfomance machines. After tuning the model, result might be much higher.

